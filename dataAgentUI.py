import os
import pandas as pd
import gradio as gr
import asyncio
from dotenv import load_dotenv
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.messages import TextMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

load_dotenv()
conversation_log = []

# è®€å–ç’°å¢ƒè®Šæ•¸ä¸­çš„ API Key
gemini_api_key = os.getenv("GEMINI_API_KEY")
model_client = OpenAIChatCompletionClient(model="gemini-2.0-flash", api_key=gemini_api_key)
# å°è©±çµ‚æ­¢æ¢ä»¶
termination_condition = TextMentionTermination("terminate")
    
# CSV æ‘˜è¦å‡½å¼
def summarize_csv(file_path, chunk_size=500):
    summaries = []
    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):
        summary = f"ã€å€å¡Š {i+1}ã€‘\nå‰3è¡Œï¼š\n{chunk.head(3).to_csv(index=False)}\nç¸½è¡Œæ•¸ï¼š{chunk.shape[0]}"
        summaries.append(summary)
    return "\n".join(summaries)

async def process_file(file_obj, chat_history):
    global conversation_log
    conversation_log.clear()
    if hasattr(file_obj, "name"):
        file_path = file_obj.name
    else:
        chat_history.append({"role": "system", "content": "ç„¡æ³•å–å¾—æª”æ¡ˆè·¯å¾‘"})
        yield chat_history, None
        return
    
    chat_history.append({"role": "system", "content": "ğŸ“Š é–‹å§‹åˆ†ææ—¥è¨˜å…§å®¹..."})
    yield chat_history, None

    chunks = list(pd.read_csv(file_path, chunksize=500))
    total_records = sum(chunk.shape[0] for chunk in chunks)
    
    # Debug è¨Šæ¯
    print("å·²æˆåŠŸè®€å– CSV æª”æ¡ˆï¼Œé–‹å§‹è™•ç†æ¯å€‹å€å¡Š...")

    for idx, chunk in enumerate(chunks):
        chat_history.append({"role": "system", "content": f"ğŸ“Œ æ­£åœ¨è™•ç†ç¬¬ {idx+1} æ‰¹è³‡æ–™..."})
        yield chat_history, None
        
        prompt = f"ç¬¬ {idx+1} æ‰¹æ¬¡æ—¥è¨˜å…§å®¹:\n{chunk.head(3).to_dict(orient='records')}\nè«‹æä¾›åˆ†æèˆ‡å»ºè­°ã€‚"
        
        agents = [
            AssistantAgent("data_agent", model_client),
            AssistantAgent("analysis_agent", model_client),
            AssistantAgent("coaching_agent", model_client),
            UserProxyAgent("user_proxy")
        ]
        
        team = RoundRobinGroupChat(agents, termination_condition)
        async for event in team.run_stream(task=prompt):
            if isinstance(event, TextMessage):
                chat_history.append({"role": "assistant", "content": event.content})
                conversation_log.append({"source": event.source, "content": event.content})
                
                print(f"ğŸ’¬ Assistant å›æ‡‰ï¼ˆå€å¡Š {idx+1}ï¼‰: {event.content}")  # Debug
                yield chat_history, None  # **ç¢ºä¿ UI æœƒå³æ™‚æ›´æ–° AI å›æ‡‰**

    # åˆ†æå®Œæˆï¼Œå„²å­˜å°è©±ç´€éŒ„
    log_file = "positive_thinking_log.csv"
    pd.DataFrame(conversation_log).to_csv(log_file, index=False, encoding="utf-8-sig")
    chat_history.append({"role": "system", "content": "âœ… åˆ†æå®Œæˆï¼"})
    print("ğŸ¯ æ‰€æœ‰å€å¡Šåˆ†æå®Œæˆï¼")
    yield chat_history, log_file


def send_user_msg(msg, chat_history):
    chat_history.append({"role": "user", "content": msg})
    return chat_history, ""

with gr.Blocks() as demo:
    gr.Markdown("### ğŸ“– æ­£å‘æ€è€ƒæ—¥è¨˜åˆ†æç³»çµ±")
    file_input = gr.File(label="ä¸Šå‚³ CSV")
    chat_display = gr.Chatbot(label="åˆ†æéç¨‹", type="messages")
    download_log = gr.File(label="ä¸‹è¼‰åˆ†æçµæœ")
    start_btn = gr.Button("é–‹å§‹åˆ†æ")
    start_btn.click(
        fn=process_file,
        inputs=[file_input, chat_display],
        outputs=[chat_display, download_log]
        )

demo.queue().launch()
